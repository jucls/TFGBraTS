\chapter{Estado del arte}

En este capítulo estudiaremos y analizaremos los diferentes enfoques para nuestra principal tarea, \textbf{la segmentación de tumores cerebrales}. Abordando desde el inicio del estudio del problema pasando por la explosión de métodos basados en Aprendizaje profundo con la constitución de \textbf{BraTS} hasta nuestros días. Se pondrá especial énfasis a las soluciones actuales comparándolas desde sus diferencias en metodología y perspectiva.

Otras tareas contenidas en el diagnóstico completo de tumores cerebrales como la clasificación de tumores cerebrales (glioblastomas y meningiomas) o la predicción de su evolución, que se recogen en este trabajo, van intrínsecamente unidas al buen desempeño de la tarea de segmentación. 

Por un lado, la clasificación entre los dos tipos de tumores no es tan relevante a la hora de diseñar un sistema de ayuda a la toma de decisión ya que clínicamente sí existe una característica diferencial entre ambos, su \textbf{localización}. Los meningiomas siempre aparecen entre el cráneo y el cerebro no internamente en el cerebro como los glioblastomas. Esto hace que un médico pueda distinguirlos sin requerir una gran asistencia de una máquina para la mayoría de los casos. Adicionalmente, la clasificación de por sí ayuda a la toma de decisiones en el tratamiento pero no es el elemento crítico para la supervivencia del paciente que depende de la eliminación del tumor donde su segmentación toma un papel crucial.

Por otro lado, la tarea de la predicción de la evolución del tumor se representa como la predicción de la segmentación en un estado futuro del tumor es decir depende fuertemente de la segmentación actual del tumor. 

Es por ello que grandes esfuerzos se han realizado en entorno a la segmentación, ya que otras tareas que conforman su diagnóstico se verían arrastradas.

Diferentes dificultades han sido las que a pesar de años de desarrollo aún encontrar un algoritmo para la segmentación de tumores cerebrales sea algo mejorable. 

\begin{enumerate}
	\item \textbf{Incertidumbre en la localización} : Como vimos no existe una zona concreta en general para la aparición de los tumores cerebrales. A excepción, de los meningiomas localizados en zonas superficiales del cerebro y aún siendo una región muy amplia, incluso ya desarrollado un tumor pueden aparecer otros localizados en regiones muy distintas de la original. 
	\item \textbf{Incertidumbre en la morfología} : A diferencia de otras patologías, cada tumor cerebral presenta un tamaño y forma completamente distintas y donde en principio no se puede apreciar un patrón distintivo. Esto hace que sea muy complicado y generalmente aporte malos resultados, la construcción de sistemas basados reglas u otras aproximaciones que no incluyen una componente de aprendizaje.
	
	\item \textbf{Bajo contraste} : Una buena resolución y contraste son características muy importantes para entender la información de una imagen. Las imágenes IRM producidas en una resonancia debido a proyecciones de imagen y procesos de tomografía usualmente ofrecen una baja resolución y contraste haciendo más difícil la definición de bordes entre diferentes tejidos de la imagen. Una segmentación precisa es difícil de conseguir.
	
	\item \textbf{Sesgo en las etiquetas}. Existen indicios para pensar que las etiquetas proporcionadas pueden presentar ruido. El proceso de segmentado por parte del personal médico 
	depende de su experiencia profesional lo cual puede llevar a cometer errores. Por ejemplo, se han presentado eventualmente discrepancias entre distintos anotadores: algunos tienden a conectar todas las pequeñas regiones de un tejido mientras que otros las segmentan de forma más precisa y separada. 
	
	\item \textbf{Desbalanceo en el tejido} : Dentro de la segmentación entre los diferentes tipos de tejidos, usualmente existe un tejido NCR que es usualmente más pequeño que los otros dos. 
	Esto podría afectar en el proceso de aprendizaje hacia una pobre generalización de este tipo de tejido. 
	
	\item \textbf{Desbalanceo entre pacientes} : En el conjunto de datos tenemos muchos pacientes de norteamerica y de ascendencia blanca, pero pocos de otros origenes como el africano. Además, de tener un sesgo claro de edad ya que existen pocos casos en niños. Esta falta de datos puede impedir que exista una buena generalización para estos casos más aislados.
	
\end{enumerate}



\section{Revisión histórica}

	A continuación, se presenta una revisión histórica sobre la segmentación de tumores cerebrales hasta 2021 apoyada en \cite{liu2023deep}. Se presenta una línea del tiempo con los principales trabajos de estudio.
	
	\begin{figure}[!h]
		\centering
		\includegraphics[width=1.0\linewidth]{imagenes/evolution_stateofart.png}
		\caption{Evolución histórica del estado del arte hasta 2021.}
	\end{figure}
	
	En la década de 1990 investigadores como \cite{zhu1997computerized} fueron pioneros al utilizar una red Hopfield con un modelo de contornos activos para extraer los bordes del tumor. Sin embargo, incluso el entrenamiento de una pequeña red como esta era algo computacionalmente costoso por las limitaciones de la época.  Desde 1990 hasta 2012, los métodos que iban surgiendo para la segmentación de tumores cerebrales estaban basados en métodos clásicos de aprendizaje con características extraídas a mano, sistemas expertos que se apoyaban en los histogramas de la imagen, plantillas para la segmentación y modelos gráficos. 
	
	A pesar de ser un gran paso inicial, tenían grandes deficiencias. Por ejemplo, la mayoría de ellos sólo se centraba en la segmentación de todo el tumor lo cual lleva a un modelo poco útil. Por otro lado, en los modelos basados en características extraídas se hacía muy tedioso poder usarlos eficazmente ya que este paso de extracción dependía de conocimiento previo experto que en ningún momento se pudo llegar a representar en un modelo. En último lugar, los mismos problemas que compartimos hoy en día sobre el desbalanceo y la incertidumbre del problema eran mucho más agresivos. 
	
	Tras 2012 con la revolución del Deep Learning, se introducen nuevas tecnologías (Redes neuronales convolucionales y U-net) que mejorarán los resultados obtenidos hasta el momento. 
	Se empezarán a construir arquitecturas encoder-decorder convolucionales para conseguir pipelines completos para la segmentación. El aprendizaje profundo toma el problema de lleno proclamándose el enfoque que define el estado del arte.
	
	Podemos clasificar las soluciones basadas en aprendizaje profundo aportadas en tres categorías según el principal problema para que el están pesadas. Sin embargo, como veremos en las soluciones más actuales lo ideal es tratar con los tres problemas.
	
	\subsection{Métodos que se enfocan en la arquitectura}
		
		 Para poder obtener redes que automáticamente extraen características discriminativas a altas dimensiones es necesario un efectivo diseño de módulos y arquitecturas. Por un lado, se pretende que la arquitectura sea capaz de aprender semánticamente y a localizar regiones de interés por medio de añadir profundidad a la red, a través de mecanismos de atención o la fusión de características entre las resonancias. Por otro lado, se pretende minimizar la cantidad de parámetros entrenables de la red o conseguir un entrenamiento más rápido.
		 
		\subsubsection{Diseño de bloques especializados}
			
			Los primeros trabajos que tenían este objetivo comenzaron por basarse en arquitecturas bien conocidas como AlexNet o VGGNet a través del uso de una secuencia de imágenes de la resonancia completa como entrada.
			
			Para la mejora de resultados, se optó por introducir todas las resonancias como entrada de la red y añadir más capas convolucionales. Con ello, teníamos redes más profundas pero que pronto empezaban a sufrir los problemas del explosión y desvanecimiento del gradiente durante el proceso de entrenamiento. Para ayudar a lidiar con estos problemas, se introdujo a las redes, \textbf{conexiones residuales}.
			Conectando la entrada de la red con su salida, convergiendo más rápido y con mejores resultados. 
			
			Este proceso de aumento de profundidad con conexiones residuales no sería definitivo porque también conlleva el sacrificio de resolución espacial. Se reemplazaría en trabajos siguientes, el uso de la convolución simple por convoluciones dilatadas. El \textbf{uso de convoluciones dilatadas} traería el aumento del espacio receptivo (ya que se aplica una convolución a un espacio mayor de la imagen) sin necesidad de introducir parámetros a la red. La convolución dilatada se vería especialmente útil por ejemplo en la segmentación de áreas grandes como suele ocupar el tejido ED (edema tumoral). 
			
			Respecto conseguir una buena eficiencia en tiempo de entrenamiento es conocido aplicar un reordenamiento en memoria de las imágenes de la resonancia similares (p. ej. el mismo slice en las 4 pruebas) de forma que reduzcan la comunicación entrada-salida con GPU. Adicionalmente, se utilizan \textbf{conexiones reversibles} en la red de forma que durante el proceso de backpropagation (backward pass) no se necesite memoria adicional para guardar las activaciones intermedias y la aplicación de la combinación \textbf{convoluciones separables} para cada convolución standard necesaria por motivos de eficiencia. 
			
			\begin{figure}[!h]
				\centering
				\includegraphics[width=1.0\linewidth]{imagenes/estructura_revisionhistorica.png}
				\caption{Estructura de las arquitecturas de esta revisión.}
			\end{figure}
			
			\subsubsection{Diseño de arquitecturas efectivas}
			
			La mayoría de los trabajos de recorrido histórico se encasillan en alguno de los siguientes dos enfoques de arquitectura: \textbf{redes neuronales convolucionales} para extraer características de la imagen y clasificar los píxeles de la imagen según las etiquetas de los tejidos posibles o \textbf{redes encoder-decoder} en las cuales se puede definir un pipeline completo convolucional sin la necesidad de la agregación de capas totalmente conectadas.
			
			\begin{enumerate}
					
				\item \textbf{Redes neuronales convolucionales de una/múltiples trayectorias}
					
					
				A diferencia de una red convolucional de una única trayectoria, las redes de trayectoria múltiples tienen la capacidad de extraer diversas características a diferentes escalas. Estas características se combinan para su posterior procesamiento, permitiendo a las redes aprender tanto características globales como locales. 
				
				\begin{figure}[!h]
					\centering
					\includegraphics[width=0.5\linewidth]{imagenes/comparisonsinglemultipleCNN.png}
					\caption{Comparación entre arquitecturas de una y múltiples trayectorias. Imagen de \cite{liu2023deep}}
				\end{figure}
				
				Por ejemplo, \cite{havaei2017brain} desarrollaron una estructura de dos vías que integra información tanto local como global del tumor, utilizando núcleos de convolución de diferentes tamaños.
				
				\begin{figure}[!h]
					\centering
					\includegraphics[width=0.75\linewidth]{imagenes/havaei2017architecture.png}
					\caption{Arquitectura de dos vías de \cite{havaei2017brain}}
				\end{figure}
				
				Otros enfoques, como el de \cite{kamnitsas2017efficient}, optan por aprender información global localmente desde la entrada misma, utilizando redes de doble vía patches de diferentes tamaños y pequeños núcleos de convolución. 
				
				Este tipo de arquitecturas fueron una de las primeras aproximaciones que empezaban adaptarse con éxito a las complejidades de la segmentación de tumores cerebrales.
				
					
				\item \textbf{Arquitecturas Encoder-Decoder}
					
			\end{enumerate}
			
			
		\subsection{Métodos que tratan el desbalanceo}
			\subsubsection{Métodos multi-red}
			\begin{enumerate}
					
				\item \textbf{Redes en cascada}
					
				\item \textbf{Ensamblado de modelos}
					
			\end{enumerate}
				
			\subsubsection{Funciones de pérdida especializadas}
		
		
		\subsection{Métodos que tratan la información multi-modal}


\section{Enfoques actuales}
	\subsection{Basados en CNN}
	
	Como vimos anteriormente históricamente existen referencias muy buenas de forma independiente por tratar de conseguir una arquitectura efectiva, evitar los problemas asociados al desbalanceo y sacarle partido a la información multi-modal. En aproximaciones más reciente se intenta unificar estos propósitos con el fin de un desempeño mayor en el problema.
	
	
	
	
	\subsection{Basados en Transformers}

